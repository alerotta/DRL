{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNj9YjlOODh7IdZjhjMKDfR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alerotta/DRL/blob/main/01%20-%20Cross%20Entropy/Cross_Entropy_FrozenLake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kyfb-uWz8Pq1"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium[toy-text] --quiet\n",
        "!pip install torch --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from dataclasses import dataclass\n",
        "import typing as tt\n",
        "import random\n",
        "from torch.utils.tensorboard.writer import SummaryWriter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n"
      ],
      "metadata": {
        "id": "Gk-JbtdQ82kq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# just an example, it is not optimized and this method is not the best even for simple envs\n",
        "# this is the proof since it is very slow.\n",
        "\n",
        "\n",
        "\n",
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 100\n",
        "PERCENTILE = 30\n",
        "GAMMA = 0.9\n",
        "\n",
        "#definition of a wrapper of the env to have one-hot encodings\n",
        "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
        "  def __init__(self, env):\n",
        "    super(DiscreteOneHotWrapper,self).__init__(env)\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    shape = (env.observation_space.n, )\n",
        "    self.observation_space = gym.spaces.Box(0.0, 1.0, shape, dtype=np.float32)\n",
        "\n",
        "  def observation (self,observation):\n",
        "     res = np.copy(self.observation_space.low)\n",
        "     res[observation] = 1.0\n",
        "     return res\n",
        "\n",
        "# network class\n",
        "class MyNet(nn.Module):\n",
        "  def __init__(self,n_input,n_hidden,n_actions):\n",
        "    super().__init__()\n",
        "    self.input_layer = nn.Linear(n_input,n_hidden)\n",
        "    self.hidden_layer = nn.Linear(n_hidden,n_actions)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.hidden_layer(F.relu(self.input_layer(x)))\n",
        "    return x\n",
        "\n",
        "@dataclass\n",
        "class EpisodeStep():\n",
        "  observation: np.array #state array from env\n",
        "  action: int #action taken\n",
        "\n",
        "@dataclass\n",
        "class Episode():\n",
        "  reward: float #total reward of the ep\n",
        "  steps: tt.List[EpisodeStep] #list of all steps of the ep\n",
        "\n",
        "def iterate_batches (env: gym.Env, net: MyNet, batch_size: int):\n",
        "  batch = []\n",
        "  episode_reward = 0.0\n",
        "  episode_steps = []\n",
        "  obs, _ = env.reset()\n",
        "  sm = nn.Softmax(dim=1)\n",
        "  while True :\n",
        "    obs_v = torch.tensor(obs,dtype=torch.float32)\n",
        "    act_probs_v = sm(net.forward(obs_v.unsqueeze(0)))\n",
        "    act_probs = act_probs_v.data.numpy()[0]\n",
        "    action = np.random.choice(len(act_probs), p=act_probs)\n",
        "    next_obs, reward, is_done, is_trunc, _ = env.step(action)\n",
        "    episode_reward += float(reward)\n",
        "    step = EpisodeStep(observation=obs,action=action)\n",
        "    episode_steps.append(step)\n",
        "    if is_done or is_trunc :\n",
        "      e = Episode(reward=episode_reward,steps=episode_steps)\n",
        "      batch.append(e)\n",
        "      episode_reward = 0.0\n",
        "      episode_steps = []\n",
        "      next_obs , _ = env.reset()\n",
        "      if len(batch) == batch_size :\n",
        "        yield batch\n",
        "        batch = []\n",
        "      obs = next_obs\n",
        "\n",
        "def filter_batch (batch: tt.List[Episode], percentile: float):\n",
        "  reward_fun = lambda s: s.reward * (GAMMA ** len(s.steps)) #discounted reward\n",
        "  disc_rewards = list(map(reward_fun,batch))\n",
        "  reward_bound = np.percentile(disc_rewards,percentile)\n",
        "\n",
        "  train_obs : tt.List[np.array] = []\n",
        "  train_act : tt.List[int] = []\n",
        "  elite_batch: tt.List[Episode] = [] #episodes saved for more the one iteration\n",
        "\n",
        "  for example, discounted_reward in zip(batch,disc_rewards):\n",
        "    if discounted_reward > reward_bound:\n",
        "      train_obs.extend(map(lambda step: step.observation, example.steps))\n",
        "      train_act.extend(map(lambda step: step.action, example.steps))\n",
        "      elite_batch.append(example)\n",
        "\n",
        "  return elite_batch, train_obs, train_act, reward_bound\n",
        "\n",
        "def train ():\n",
        "  random.seed(12345)\n",
        "  env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v1\"))\n",
        "  obs_size = env.observation_space.shape[0]\n",
        "  n_actions = env.action_space.n\n",
        "\n",
        "  net = MyNet(obs_size, HIDDEN_SIZE, n_actions)\n",
        "  loss = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(params = net.parameters(), lr= 0.001)\n",
        "  writer = SummaryWriter(comment=\"-frozenlake-tweaked\")\n",
        "\n",
        "  full_batch = []\n",
        "  for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
        "    reward_mean = float(np.mean(list(map(lambda s: s.reward, batch))))\n",
        "    full_batch, obs, acts, reward_bound = filter_batch(full_batch + batch, PERCENTILE)\n",
        "    if not full_batch:\n",
        "      continue\n",
        "    obs_v = torch.FloatTensor(np.vstack(obs))\n",
        "    acts_v = torch.LongTensor(acts)\n",
        "    full_batch = full_batch[-500:]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    action_scores_v = net(obs_v)\n",
        "    loss_v = loss(action_scores_v, acts_v)\n",
        "    loss_v.backward()\n",
        "    optimizer.step()\n",
        "    print(\"%d: loss=%.3f, rw_mean=%.3f, \"\n",
        "              \"rw_bound=%.3f, batch=%d\" % (\n",
        "            iter_no, loss_v.item(), reward_mean,\n",
        "            reward_bound, len(full_batch)))\n",
        "    writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
        "    writer.add_scalar(\"reward_mean\", reward_mean, iter_no)\n",
        "    writer.add_scalar(\"reward_bound\", reward_bound, iter_no)\n",
        "    if reward_mean > 0.8:\n",
        "      print(\"Solved!\")\n",
        "      break\n",
        "  writer.close()\n",
        "\n",
        "\n",
        "train()"
      ],
      "metadata": {
        "id": "LqYP0qY3HB1U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRKLjXD7rQAUZbIDzYZ7q6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alerotta/DRL/blob/main/01%20-%20Cross%20Entropy/Cross_entropy_cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[classic-control] --quiet\n",
        "!pip install torch --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6VXOp9vam6I",
        "outputId": "ae72ce49-768f-4b29-e13a-01cbc972ba4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from dataclasses import dataclass # this is a decorator that simplify creatig class process.\n",
        "import typing as tt # module to add type hints\n",
        "from torch.utils.tensorboard.writer import SummaryWriter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "mV8r0Td7bDvn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HIDDEN_SIZE = 256 #this is the number of hidden units.\n",
        "BATCH_SIZE = 16 #this is the number of episodes played before each update\n",
        "PERCENTILE = 70 #this is the percentile of discrded episodes\n",
        "\n",
        "## N.N. definition\n",
        "# This is a simple nn, we do not need anything to complex for this example,\n",
        "# two layers and a relu.\n",
        "\n",
        "class MyNet(nn.Module) :\n",
        "  def __init__(self, obs_size, hidden_size,n_actions):\n",
        "    super().__init__()\n",
        "    self.input_layer = nn.Linear(obs_size,hidden_size)\n",
        "    self.hidden_layer = nn.Linear(hidden_size,n_actions)\n",
        "\n",
        "  def forward (self,x):\n",
        "    x = self.hidden_layer(nn.functional.relu(self.input_layer(x)))\n",
        "\n",
        "    return x\n",
        "\n",
        "## definition of classes used to contain data of learing phase\n",
        "\n",
        "@dataclass\n",
        "class EpisodeStep:\n",
        "  observation: np.ndarray #this is the state of the env, it will be given by gym\n",
        "  action: int #this is the action taken\n",
        "\n",
        "@dataclass\n",
        "class Episode:\n",
        "  reward: float #the total reward of the episode\n",
        "  steps: tt.List[EpisodeStep] #the list of all ep. steps\n",
        "\n",
        "## function to 'play' the episode batch.\n",
        "\n",
        "def run_batches(environment, network, batch_size) -> tt.Generator[tt.List[Episode],None,None]: # -> used for return type specification\n",
        "  batch = []\n",
        "  episode_reward = 0.0\n",
        "  episode_steps = []\n",
        "  obs, _ = environment.reset() # numpy vector\n",
        "  softmax = nn.Softmax(dim=1) # softmax layer\n",
        "  while True:\n",
        "    obs_v = torch.tensor(obs, dtype=torch.float32) # pythorch tensor casting\n",
        "    act_probs_v = softmax(network(obs_v.unsqueeze(0))) # action prob vector from the network\n",
        "    act_probs = act_probs_v.data.numpy()[0] # casting to numpy\n",
        "    action = np.random.choice(len(act_probs), p=act_probs) # uses numpy to take a random action\n",
        "    next_obs,reward,is_done,is_trunc,_ = environment.step(action) # act\n",
        "    episode_reward += float(reward) # comulative reward (not discounted)\n",
        "    step = EpisodeStep(obs,action) # save data\n",
        "    episode_steps.append(step) # save data\n",
        "    if is_done or is_trunc :\n",
        "      e = Episode(episode_reward,episode_steps)\n",
        "      batch.append(e)\n",
        "      episode_reward= 0.0\n",
        "      episode_steps = []\n",
        "      next_obs,_ = environment.reset()\n",
        "      if len(batch) == batch_size:\n",
        "        yield batch\n",
        "        batch = []\n",
        "    obs = next_obs\n",
        "\n",
        "## function to discard 'bad' episodes and create data for the training\n",
        "\n",
        "def filter_episode (batch,percentile):\n",
        "  rewards = list(map(lambda s: s.reward , batch)) # extract reward of each episode and create a list\n",
        "  reward_bound = float (np.percentile(rewards,percentile)) # find the threshold given the percentile\n",
        "  reward_mean = float(np.mean(rewards)) # mean of the rewards of the bact\n",
        "\n",
        "  train_obs: tt.List[np.ndarray] = []\n",
        "  train_act: tt.List[int] = []\n",
        "  for episode in batch:\n",
        "    if episode.reward < reward_bound :\n",
        "      continue\n",
        "    train_obs.extend(map(lambda step: step.observation, episode.steps))\n",
        "    train_act.extend(map(lambda step: step.action, episode.steps))\n",
        "\n",
        "  train_obs_v = torch.FloatTensor(np.vstack(train_obs)) #casting\n",
        "  train_act_v = torch.LongTensor(train_act) #casting\n",
        "  return train_obs_v, train_act_v, reward_bound, reward_mean\n",
        "\n",
        "\n",
        "def train ():\n",
        "  env = gym.make(\"CartPole-v1\") #create environment\n",
        "  obs_size = env.observation_space.shape[0] #take the observation size\n",
        "  n_actions = int(env.action_space.n) # number of possible actions\n",
        "\n",
        "  net = MyNet(obs_size,HIDDEN_SIZE,n_actions) #instance the network\n",
        "  objective = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
        "  writer = SummaryWriter(comment=\"-cartpole\")\n",
        "\n",
        "  for iter_no, batch in enumerate(run_batches(env, net, BATCH_SIZE)):\n",
        "    obs_v, acts_v, reward_b, reward_m = filter_episode(batch, PERCENTILE)\n",
        "    optimizer.zero_grad()\n",
        "    action_scores_v = net.forward(obs_v)\n",
        "    loss_v = objective(action_scores_v,acts_v)\n",
        "    loss_v.backward()\n",
        "    optimizer.step()\n",
        "    print(\"%d: loss=%.3f, reward_mean=%.1f, rw_bound=%.1f\" % (\n",
        "            iter_no, loss_v.item(), reward_m, reward_b))\n",
        "    writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
        "    writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
        "    writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
        "    if reward_m > 475:\n",
        "      print(\"Solved!\")\n",
        "      break\n",
        "  writer.close()\n",
        "\n",
        "\n",
        "\n",
        "train()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HCgfZ2OPqps",
        "outputId": "c72b07a7-aba9-425f-b4a1-46cc7edf909a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: loss=0.685, reward_mean=17.1, rw_bound=18.5\n",
            "1: loss=0.671, reward_mean=27.1, rw_bound=29.5\n",
            "2: loss=0.647, reward_mean=36.7, rw_bound=41.5\n",
            "3: loss=0.648, reward_mean=37.6, rw_bound=44.0\n",
            "4: loss=0.613, reward_mean=41.0, rw_bound=47.0\n",
            "5: loss=0.605, reward_mean=48.6, rw_bound=58.0\n",
            "6: loss=0.589, reward_mean=58.2, rw_bound=64.0\n",
            "7: loss=0.566, reward_mean=48.8, rw_bound=50.5\n",
            "8: loss=0.558, reward_mean=61.1, rw_bound=62.5\n",
            "9: loss=0.552, reward_mean=75.9, rw_bound=74.0\n",
            "10: loss=0.528, reward_mean=63.9, rw_bound=72.0\n",
            "11: loss=0.535, reward_mean=74.7, rw_bound=77.0\n",
            "12: loss=0.532, reward_mean=91.1, rw_bound=101.0\n",
            "13: loss=0.495, reward_mean=106.8, rw_bound=121.0\n",
            "14: loss=0.507, reward_mean=108.8, rw_bound=130.0\n",
            "15: loss=0.491, reward_mean=85.1, rw_bound=88.5\n",
            "16: loss=0.492, reward_mean=94.4, rw_bound=119.5\n",
            "17: loss=0.473, reward_mean=104.6, rw_bound=115.5\n",
            "18: loss=0.477, reward_mean=127.0, rw_bound=139.5\n",
            "19: loss=0.483, reward_mean=176.8, rw_bound=192.5\n",
            "20: loss=0.466, reward_mean=176.7, rw_bound=183.5\n",
            "21: loss=0.464, reward_mean=162.3, rw_bound=167.0\n",
            "22: loss=0.468, reward_mean=166.2, rw_bound=167.0\n",
            "23: loss=0.464, reward_mean=174.4, rw_bound=183.0\n",
            "24: loss=0.475, reward_mean=150.4, rw_bound=160.0\n",
            "25: loss=0.456, reward_mean=164.8, rw_bound=154.5\n",
            "26: loss=0.440, reward_mean=150.8, rw_bound=149.5\n",
            "27: loss=0.454, reward_mean=176.6, rw_bound=187.0\n",
            "28: loss=0.451, reward_mean=189.1, rw_bound=221.5\n",
            "29: loss=0.452, reward_mean=236.4, rw_bound=268.0\n",
            "30: loss=0.449, reward_mean=257.5, rw_bound=298.5\n",
            "31: loss=0.445, reward_mean=283.3, rw_bound=325.0\n",
            "32: loss=0.446, reward_mean=307.1, rw_bound=367.0\n",
            "33: loss=0.447, reward_mean=362.1, rw_bound=418.5\n",
            "34: loss=0.432, reward_mean=423.1, rw_bound=500.0\n",
            "35: loss=0.434, reward_mean=439.4, rw_bound=500.0\n",
            "36: loss=0.440, reward_mean=483.1, rw_bound=500.0\n",
            "Solved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bCi1BP3rSuZA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
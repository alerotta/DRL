{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpZRJhz4hwXoIk+7xHo8mr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alerotta/DRL/blob/main/02%20-%20Q%20Learning/Tabular_Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[toy-text] --quiet\n",
        "!pip install torch --quiet"
      ],
      "metadata": {
        "id": "NBSmP46lyLu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VpTKyUrLxZbN"
      },
      "outputs": [],
      "source": [
        "import typing as tt\n",
        "import gymnasium as gym\n",
        "from collections import defaultdict\n",
        "from torch.utils.tensorboard.writer import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ENV_NAME = \"FrozenLake-v1\"\n",
        "GAMMA = 0.9\n",
        "ALPHA =0.2\n",
        "TEST_EPISODES = 20\n",
        "\n",
        "state = int\n",
        "action = int\n",
        "ValueKey = tt.Tuple[state,action]\n",
        "\n",
        "class Agent ():\n",
        "  def __init__(self):\n",
        "    self.env = gym.make(ENV_NAME)\n",
        "    self.state, _ = self.env.reset()\n",
        "    self.values : tt.Dict[ValueKey] = defaultdict(float) #the q-table, is a dictionraty where keys are state, value pairs\n",
        "\n",
        "\n",
        "  # this is random sampling and returns s,a,r,s'\n",
        "  def sample_env(self):\n",
        "    action = self.env.action_space.sample() #random action\n",
        "    old_state = self.state\n",
        "    new_state, reward , is_done, is_trunc, _ = self.env.step(action)\n",
        "    if is_done or is_trunc :\n",
        "      self.state, _ = self.env.reset()\n",
        "    else:\n",
        "      self.state = new_state\n",
        "    return old_state , action , float(reward), new_state\n",
        "\n",
        "  # this is a greegy policy selectiong best Q values.\n",
        "  def best_value_and_action (self, state):\n",
        "    best_value , best_action = None , None\n",
        "    for action in range(self.env.action_space.n) :\n",
        "      action_value = self.values[(state,action)]\n",
        "      if best_value is None or best_value < action_value:\n",
        "        best_value = action_value\n",
        "        best_action = action\n",
        "    return best_value, best_action\n",
        "\n",
        "  # this is a smooth update rule to avoid large updates.\n",
        "  def value_update(self, state, action ,reward, next_state):\n",
        "     best_val , _ =  self.best_value_and_action(next_state)\n",
        "     new_val = reward + GAMMA * best_val\n",
        "     old_val = self.values[(state,action)]\n",
        "     key = (state,action)\n",
        "     self.values[key] = old_val * (1 -ALPHA) + new_val * ALPHA\n",
        "\n",
        "  # play the ep, using the greedy policy and no updates\n",
        "  def play_episode (self, env):\n",
        "    total_reward = 0.0\n",
        "    state, _ = env.reset()\n",
        "    while True :\n",
        "      _ , action = self.best_value_and_action(state)\n",
        "      new_state, reward, is_done, is_trunc , _ = env.step(action)\n",
        "      total_reward += reward\n",
        "      if is_done or is_trunc:\n",
        "        break\n",
        "      state = new_state\n",
        "    return total_reward\n",
        "\n",
        "def run():\n",
        "  test_env = gym.make(ENV_NAME)\n",
        "  agent = Agent()\n",
        "  writer = SummaryWriter(comment=\"-q-learning\")\n",
        "\n",
        "  iter_no = 0\n",
        "  best_reward = 0.0\n",
        "  while True:\n",
        "    iter_no +=1\n",
        "    state, action , reward , next_state = agent.sample_env()\n",
        "    agent.value_update(state,action,reward,next_state)\n",
        "\n",
        "    test_reward = 0.0\n",
        "\n",
        "    for _ in range(TEST_EPISODES):\n",
        "      test_reward += agent.play_episode(test_env)\n",
        "    test_reward /= TEST_EPISODES #mean\n",
        "    writer.add_scalar(\"reward\", test_reward, iter_no)\n",
        "    if test_reward > best_reward:\n",
        "      print(\"%d: Best test reward updated %.3f -> %.3f\" % (iter_no, best_reward, test_reward))\n",
        "      best_reward = test_reward\n",
        "    if test_reward > 0.8:\n",
        "      print(\"Solved in %d iterations!\" % iter_no)\n",
        "      break\n",
        "\n",
        "  writer.close()\n",
        "\n",
        "run()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5lY5MZAyWpq",
        "outputId": "f9f6e3ef-5c68-4914-c8cd-cd25925f69f9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "433: Best test reward updated 0.000 -> 0.300\n",
            "436: Best test reward updated 0.300 -> 0.350\n",
            "439: Best test reward updated 0.350 -> 0.450\n",
            "1604: Best test reward updated 0.450 -> 0.500\n",
            "2614: Best test reward updated 0.500 -> 0.600\n",
            "3060: Best test reward updated 0.600 -> 0.700\n",
            "3281: Best test reward updated 0.700 -> 0.850\n",
            "Solved in 3281 iterations!\n"
          ]
        }
      ]
    }
  ]
}